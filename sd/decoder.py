import torch
from torch import nn
from torch.nn import functional as F
from attention import SelfAttention

'''
    ### decoder.py Explained ###

This script implements the decoder component of the Variational Autoencoder (VAE), which includes the VAE_AttentionBlocks, VAE_ResidualBlocks, and the decoder itself.

The main objective of this script is to perform the inverse of the encoder. It takes the latent representation generated by the encoder and reconstructs it back into the original image space. This involves reducing the number of channels while simultaneously increasing the spatial dimensions of the image, effectively transforming the compressed latent representation back into a high-dimensional image.

The decoder utilizes a combination of upsampling, convolutions, residual connections, and attention mechanisms to achieve this transformation, ensuring that the reconstructed image retains as much fidelity to the original as possible.
'''


class VAE_AttentionBlock(nn.Module):
    '''
    VAE_AttentionBlock applies self-attention mechanisms to capture dependencies across 
    different parts of the image.

    Attributes:
        groupnorm (nn.GroupNorm): Normalization layer to stabilize training.
        attention (SelfAttention): Self-attention mechanism to enhance feature representation.
    '''
    def __init__(self, channels):
        '''
        Initializes the VAE_AttentionBlock with normalization and self-attention layers.

        Args:
            channels (int): Number of input channels.
        '''
        super().__init__()
        self.groupnorm = nn.GroupNorm(32, channels)
        self.attention = SelfAttention(1, channels)

    def forward(self, x):
        '''
        Forward pass through the attention block.

        Args:
            x (torch.Tensor): Input tensor with shape (batch size, features, height, width).

        Returns:
            torch.Tensor: Output tensor with enhanced feature representation.
        '''
        #x: (batch size, features, height, width)

        residue = x

        n, c, h, w = x.shape

        #(batch size, features, height, width) -> (batch size, features, height*width)
        x = x.view((n, c, h*w))

        #(batch size, features, height*width) -> (batch size, height*width, features)
        # each pixel becomes a feature of size features, the sequence length is height*width, 
        x = x.transpose(-1, -2)

        # perform self attention without mask
        # (batch size, height*width, features) -> (batch size, height*width, features)
        x = self.attention(x)

        # (batch size, height*width, features) -> (batch size, features, height*width)
        x = x.transpose(-1, -2)

        #(batch size, features, height*width) -> (batch size, features, height, width)
        x = x.view((n, c, h, w))

        x += residue
        

        return x

class VAE_ResidualBlock(nn.Module):
    '''
    VAE_ResidualBlock consists of normalization and convolution layers to facilitate 
    information flow and maintain dimensional consistency.

    Attributes:
        groupnorm_1 (nn.GroupNorm): First normalization layer.
        conv_1 (nn.Conv2d): First convolution layer.
        groupnorm_2 (nn.GroupNorm): Second normalization layer.
        conv_2 (nn.Conv2d): Second convolution layer.
        residual_layer (nn.Module): Identity or convolution layer for residual connection.
    '''
    def __init__(self, in_channels, out_channels):
        '''
        Initializes the VAE_ResidualBlock with specified input and output channels. This block 
        consists of group normalization layers followed by convolutional layers. It also 
        includes a residual connection which either passes the input directly or adjusts 
        the number of channels to match the output.

        Args:
            in_channels (int): Number of input channels.
            out_channels (int): Number of output channels.

        If the number of input channels is equal to the number of output channels, the 
        residual connection is an identity mapping (nn.Identity()), meaning it directly 
        passes the input through without any changes. 
        
        If the number of input channels is different from the number of output channels, the residual connection 
        uses a 1x1 convolution (nn.Conv2d) to adjust the number of channels to match the output. 
        This ensures that the residual connection can be added to the output of the 
        convolutional layers, maintaining dimensional consistency.
        '''
        super().__init__()
        self.groupnorm_1 = nn.GroupNorm(32, in_channels)
        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)

        self.groupnorm_2 = nn.GroupNorm(32, out_channels)
        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1)

        if in_channels == out_channels:
            self.residual_layer = nn.Identity()
        else:
            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size = 1, padding = 0)

    
    def forward(self, x):
        '''
        Forward pass through the residual block.

        Args:
            x (torch.Tensor): Input tensor with shape (batch size, in_channels, height, width).

        Returns:
            torch.Tensor: Output tensor with the same shape, incorporating the residual connection.
        '''
        # x: (batch_size, in_channels, height, width)
        residue = x
        #(batch_size, in_channels, height, width) -> (batch__size, in_channels, height, width)
        x = self.groupnorm_1(x)

        #(batch_size, in_channels, height, width) -> (batch__size, in_channels, height, width)
        x = F.silu(x)

        #(batch__size, in_channels, height, width) -> (batch__size, in_channels, height, width)
        x = self.conv_1(x)

        #(batch__size, in_channels, height, width) -> (batch__size, in_channels, height, width)
        x = self.groupnorm_2(x)
        
        x = F.silu(x)

        x = self.conv_2(x)

        return x + self.residual_layer(residue)
    

class VAE_Decoder(nn.Sequential):
    '''
    VAE_Decoder reconstructs the image from the latent representation by increasing spatial 
    dimensions and reducing the number of channels.

    This class defines a sequence of upsampling, convolutional, residual, and attention blocks 
    to effectively decode the latent representation back into the original image space.
    '''
    def __init__(self):
        '''
        Initializes the VAE_Decoder with a sequence of layers designed to reconstruct the image.
        '''
        super().__init__(
            # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)
            nn.Conv2d(4, 4, kernel_size = 1, padding = 0),

            # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)
            nn.Conv2d(4, 512, kernel_size = 3, padding = 1),

            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)
            VAE_ResidualBlock(512, 512),

            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)
            VAE_AttentionBlock(512),

            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)
            VAE_ResidualBlock(512, 512),

            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)
            VAE_ResidualBlock(512, 512),

            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)
            VAE_ResidualBlock(512, 512),

            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)
            VAE_ResidualBlock(512, 512),

            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 4, Width / 4)
            nn.Upsample(scale_factor=2),

            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)
            nn.Conv2d(512, 512, kernel_size = 3, padding = 1),

            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)
            VAE_ResidualBlock(512, 512),
            VAE_ResidualBlock(512, 512),
            VAE_ResidualBlock(512, 512),

            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 2, Width / 2)
            nn.Upsample(scale_factor=2),

            # (Batch_Size, 512, Height / 2, Width / 2) -> (Batch_Size, 512, Height / 2, Width / 2)
            nn.Conv2d(512, 512, kernel_size = 3, padding = 1),


            # (Batch_Size, 512, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)
            VAE_ResidualBlock(512, 256),
            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)
            VAE_ResidualBlock(256, 256),
            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)
            VAE_ResidualBlock(256, 256),

            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height, Width)
            nn.Upsample(scale_factor=2),

            # (Batch_Size, 256, Height, Width) -> (Batch_Size, 256, Height, Width )
            nn.Conv2d(256, 256, kernel_size = 3, padding = 1),

            # (Batch_Size, 256, Height, Width) -> (Batch_Size, 128, Height, Width )
            VAE_ResidualBlock(256, 128),
             # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width )
            VAE_ResidualBlock(128, 128),
            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width )
            VAE_ResidualBlock(128, 128),
            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width )
            nn.GroupNorm(32, 128),
            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width )
            nn.SiLU(),
            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 3, Height, Width )
            nn.Conv2d(128, 3, kernel_Size = 3, padding = 1),

        )

    def forward(self, x):
        '''
        Forward pass through the decoder.

        Args:
            x (torch.Tensor): Latent representation with shape (batch size, 4, height/8, width/8).

        Returns:
            torch.Tensor: Reconstructed image with shape (batch size, 3, height, width).
        '''
        # x: (batch_size, 4, height/8, width/8)
    
        # remvoe the scaling added by encoder

        x /= 0.18215

        for module in self:
            x = module(x)e

        # (batch_size, 3, height, width)
        return x
